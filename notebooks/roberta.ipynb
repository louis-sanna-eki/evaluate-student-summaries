{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34be9310",
   "metadata": {
    "id": "5500c3a6",
    "papermill": {
     "duration": 0.011551,
     "end_time": "2023-08-26T14:57:53.066259",
     "exception": false,
     "start_time": "2023-08-26T14:57:53.054708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aae8166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T14:57:53.087483Z",
     "iopub.status.busy": "2023-08-26T14:57:53.086665Z",
     "iopub.status.idle": "2023-08-26T14:57:53.099059Z",
     "shell.execute_reply": "2023-08-26T14:57:53.098173Z"
    },
    "id": "0b1fa18d",
    "papermill": {
     "duration": 0.025461,
     "end_time": "2023-08-26T14:57:53.101138",
     "exception": false,
     "start_time": "2023-08-26T14:57:53.075677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3815895",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T14:57:53.124543Z",
     "iopub.status.busy": "2023-08-26T14:57:53.123677Z",
     "iopub.status.idle": "2023-08-26T14:57:53.128851Z",
     "shell.execute_reply": "2023-08-26T14:57:53.127791Z"
    },
    "id": "6ebb2cdb",
    "papermill": {
     "duration": 0.019509,
     "end_time": "2023-08-26T14:57:53.130971",
     "exception": false,
     "start_time": "2023-08-26T14:57:53.111462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ENV = \"kaggle\"\n",
    "if 'COLAB_JUPYTER_TOKEN' in os.environ:\n",
    "    ENV = \"colab\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2763edd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T14:57:53.153393Z",
     "iopub.status.busy": "2023-08-26T14:57:53.153033Z",
     "iopub.status.idle": "2023-08-26T14:57:53.159483Z",
     "shell.execute_reply": "2023-08-26T14:57:53.158338Z"
    },
    "id": "b1f2e114",
    "outputId": "73222576-ed5d-40af-ad0d-fa05de938f65",
    "papermill": {
     "duration": 0.020987,
     "end_time": "2023-08-26T14:57:53.161949",
     "exception": false,
     "start_time": "2023-08-26T14:57:53.140962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "libraries = [\n",
    "    'functools',\n",
    "    'gc',\n",
    "    'gzip',\n",
    "    'hashlib',\n",
    "    'kaggle',\n",
    "    'matplotlib',\n",
    "    'numpy',\n",
    "    'os',\n",
    "    'pandas',\n",
    "    'scikit-learn',\n",
    "    'sentencepiece',\n",
    "    'torch',\n",
    "    'tqdm',\n",
    "    'transformers==4.30.2',\n",
    "    'lightgbm'\n",
    "]\n",
    "\n",
    "if ENV == \"colab\":\n",
    "    import subprocess\n",
    "    for lib in libraries:\n",
    "        subprocess.run([\"pip\", \"install\", lib])\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f253bb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T14:57:53.182378Z",
     "iopub.status.busy": "2023-08-26T14:57:53.182087Z",
     "iopub.status.idle": "2023-08-26T14:57:59.157204Z",
     "shell.execute_reply": "2023-08-26T14:57:59.156072Z"
    },
    "id": "5687a953",
    "papermill": {
     "duration": 5.988192,
     "end_time": "2023-08-26T14:57:59.159797",
     "exception": false,
     "start_time": "2023-08-26T14:57:53.171605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)            # Set seed for python's built-in random\n",
    "    np.random.seed(seed_value)         # Set seed for numpy\n",
    "    torch.manual_seed(seed_value)      # Set seed for pytorch\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  # Use any number as seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd33c3",
   "metadata": {
    "id": "6263a77d",
    "papermill": {
     "duration": 0.009559,
     "end_time": "2023-08-26T14:57:59.179301",
     "exception": false,
     "start_time": "2023-08-26T14:57:59.169742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef85998c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T14:57:59.202070Z",
     "iopub.status.busy": "2023-08-26T14:57:59.200964Z",
     "iopub.status.idle": "2023-08-26T14:57:59.340079Z",
     "shell.execute_reply": "2023-08-26T14:57:59.339018Z"
    },
    "id": "df53320d",
    "papermill": {
     "duration": 0.152885,
     "end_time": "2023-08-26T14:57:59.342499",
     "exception": false,
     "start_time": "2023-08-26T14:57:59.189614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE_NAMES = ['prompts_train.csv', 'summaries_train.csv', 'prompts_test.csv', 'summaries_test.csv']\n",
    "\n",
    "def read_files(base_path, file_names=FILE_NAMES):\n",
    "    \"\"\"Read multiple CSV files from a given path.\"\"\"\n",
    "    dataframes = []\n",
    "    for file in file_names:\n",
    "        dataframes.append(pd.read_csv(base_path + file))\n",
    "    return tuple(dataframes)\n",
    "\n",
    "def load_data_in_kaggle():\n",
    "    \"\"\"Load data when running in a Kaggle environment.\"\"\"\n",
    "    base_path = '/kaggle/input/commonlit-evaluate-student-summaries/'\n",
    "    return read_files(base_path)\n",
    "\n",
    "def load_data_in_colab():\n",
    "    \"\"\"Load data when running in a Google Colab environment.\"\"\"\n",
    "    base_path = \"/content/drive/MyDrive/kaggle/commonlit-evaluate-student-summaries/\"\n",
    "    return read_files(base_path)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load data based on the environment (Kaggle/Colab).\"\"\"\n",
    "    if ENV == \"kaggle\":\n",
    "        return load_data_in_kaggle()\n",
    "    elif ENV == \"colab\":\n",
    "        return load_data_in_colab()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown environment.\")\n",
    "\n",
    "df_train_prompts, df_train_summaries, df_test_prompts, df_test_summaries = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f1d63b",
   "metadata": {
    "id": "36e656a3",
    "papermill": {
     "duration": 0.009474,
     "end_time": "2023-08-26T14:57:59.362302",
     "exception": false,
     "start_time": "2023-08-26T14:57:59.352828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Merge set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9dbff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T14:57:59.383804Z",
     "iopub.status.busy": "2023-08-26T14:57:59.383132Z",
     "iopub.status.idle": "2023-08-26T14:58:00.290881Z",
     "shell.execute_reply": "2023-08-26T14:58:00.289782Z"
    },
    "id": "dc3b6062",
    "outputId": "58a80a89-92fc-48bc-ddf0-3449329e5b21",
    "papermill": {
     "duration": 0.922177,
     "end_time": "2023-08-26T14:58:00.294209",
     "exception": false,
     "start_time": "2023-08-26T14:57:59.372032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape: (7165, 16)\n",
      "df_test.shape: (4, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>lexical_similarity</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>predicted_content</th>\n",
       "      <th>predicted_wording</th>\n",
       "      <th>transformer_content</th>\n",
       "      <th>transformer_wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020ae56ffbf</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.548304</td>\n",
       "      <td>0.506755</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.426385</td>\n",
       "      <td>-0.756804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.941234</td>\n",
       "      <td>-0.216973</td>\n",
       "      <td>-0.941234</td>\n",
       "      <td>-0.216973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0071d51dab6d</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would use chemicals and substances to cha...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>They would use chemicals and substances to cha...</td>\n",
       "      <td>-0.594609</td>\n",
       "      <td>-1.002909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.443029</td>\n",
       "      <td>-0.047777</td>\n",
       "      <td>-0.443029</td>\n",
       "      <td>-0.047777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00746c7c79c3</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>Many times the factories would, according to t...</td>\n",
       "      <td>-0.878889</td>\n",
       "      <td>-0.966330</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>Many times the factories would, according to t...</td>\n",
       "      <td>-0.538534</td>\n",
       "      <td>-0.812601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.005438</td>\n",
       "      <td>-0.703931</td>\n",
       "      <td>-1.005438</td>\n",
       "      <td>-0.703931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>008db54e7cbc</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>The factory covered up and used spolied meat i...</td>\n",
       "      <td>1.216547</td>\n",
       "      <td>1.166914</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>The factory covered up and used spolied meat i...</td>\n",
       "      <td>0.620339</td>\n",
       "      <td>-0.410604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866758</td>\n",
       "      <td>0.799542</td>\n",
       "      <td>0.866758</td>\n",
       "      <td>0.799542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00b8461e9c37</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>The factory would rub up the spoiled meat with...</td>\n",
       "      <td>-1.236282</td>\n",
       "      <td>-0.285223</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>The factory would rub up the spoiled meat with...</td>\n",
       "      <td>-0.688066</td>\n",
       "      <td>-0.571339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.713290</td>\n",
       "      <td>-0.695766</td>\n",
       "      <td>-0.713290</td>\n",
       "      <td>-0.695766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n",
       "1  0071d51dab6d    ebad26  They would use chemicals and substances to cha...   \n",
       "2  00746c7c79c3    ebad26  Many times the factories would, according to t...   \n",
       "3  008db54e7cbc    ebad26  The factory covered up and used spolied meat i...   \n",
       "4  00b8461e9c37    ebad26  The factory would rub up the spoiled meat with...   \n",
       "\n",
       "    content   wording                                    prompt_question  \\\n",
       "0 -0.548304  0.506755  Summarize the various ways the factory would u...   \n",
       "1  0.205683  0.380538  Summarize the various ways the factory would u...   \n",
       "2 -0.878889 -0.966330  Summarize the various ways the factory would u...   \n",
       "3  1.216547  1.166914  Summarize the various ways the factory would u...   \n",
       "4 -1.236282 -0.285223  Summarize the various ways the factory would u...   \n",
       "\n",
       "              prompt_title                                        prompt_text  \\\n",
       "0  Excerpt from The Jungle  With one member trimming beef in a cannery, an...   \n",
       "1  Excerpt from The Jungle  With one member trimming beef in a cannery, an...   \n",
       "2  Excerpt from The Jungle  With one member trimming beef in a cannery, an...   \n",
       "3  Excerpt from The Jungle  With one member trimming beef in a cannery, an...   \n",
       "4  Excerpt from The Jungle  With one member trimming beef in a cannery, an...   \n",
       "\n",
       "                                           full_text  word_count  \\\n",
       "0  They would rub it up with soda to make the sme...   -0.426385   \n",
       "1  They would use chemicals and substances to cha...   -0.594609   \n",
       "2  Many times the factories would, according to t...   -0.538534   \n",
       "3  The factory covered up and used spolied meat i...    0.620339   \n",
       "4  The factory would rub up the spoiled meat with...   -0.688066   \n",
       "\n",
       "   lexical_similarity  semantic_similarity  predicted_content  \\\n",
       "0           -0.756804                  0.0          -0.941234   \n",
       "1           -1.002909                  0.0          -0.443029   \n",
       "2           -0.812601                  0.0          -1.005438   \n",
       "3           -0.410604                  0.0           0.866758   \n",
       "4           -0.571339                  0.0          -0.713290   \n",
       "\n",
       "   predicted_wording  transformer_content  transformer_wording  \n",
       "0          -0.216973            -0.941234            -0.216973  \n",
       "1          -0.047777            -0.443029            -0.047777  \n",
       "2          -0.703931            -1.005438            -0.703931  \n",
       "3           0.799542             0.866758             0.799542  \n",
       "4          -0.695766            -0.713290            -0.695766  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train_summaries.merge(df_train_prompts, on='prompt_id')\n",
    "df_test = df_test_summaries.merge(df_test_prompts, on='prompt_id')\n",
    "\n",
    "df_test[\"wording\"] = 0.0\n",
    "df_test[\"content\"] = 0.0\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if ENV == \"colab\":\n",
    "    file_path = 'drive/MyDrive/kaggle/colab/oof_predictions.csv'\n",
    "    if os.path.exists(file_path):\n",
    "        df_train = pd.read_csv(file_path)\n",
    "\n",
    "if ENV == \"kaggle\":\n",
    "    file_path = '/kaggle/input/summaries-features-2023-08-24-17-06-57/oof_predictions.csv'\n",
    "    if os.path.exists(file_path):\n",
    "        df_train = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"df_train.shape: {df_train.shape}\")\n",
    "print(f\"df_test.shape: {df_test.shape}\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f109c",
   "metadata": {
    "id": "ce761fd0",
    "papermill": {
     "duration": 0.009882,
     "end_time": "2023-08-26T14:58:00.314493",
     "exception": false,
     "start_time": "2023-08-26T14:58:00.304611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a773b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T14:58:00.337533Z",
     "iopub.status.busy": "2023-08-26T14:58:00.335986Z",
     "iopub.status.idle": "2023-08-26T14:58:06.884079Z",
     "shell.execute_reply": "2023-08-26T14:58:06.882961Z"
    },
    "id": "ccda4514",
    "papermill": {
     "duration": 6.562694,
     "end_time": "2023-08-26T14:58:06.887426",
     "exception": false,
     "start_time": "2023-08-26T14:58:00.324732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a separator token for BERT\n",
    "separator_token = ' [SEP] '\n",
    "\n",
    "# Create the \"full_text\" feature by joining \"prompt_text\" and \"text\"\n",
    "df_train['full_text'] = df_train['text'] + separator_token + df_train['prompt_question']\n",
    "df_test['full_text'] = df_test['text'] + separator_token + df_test['prompt_question']\n",
    "\n",
    "df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "import gzip\n",
    "\n",
    "def calculate_compressed_size(text):\n",
    "    text_bytes = text.encode('utf-8')\n",
    "    compressed_bytes = gzip.compress(text_bytes)\n",
    "    return len(compressed_bytes)\n",
    "\n",
    "df_train['lexical_similarity'] = df_train.apply(\n",
    "    lambda row: 1 - calculate_compressed_size(row['text'] + row['prompt_text']) / (calculate_compressed_size(row['text']) + calculate_compressed_size(row['prompt_text'])),\n",
    "    axis=1\n",
    ")\n",
    "df_test['lexical_similarity'] = df_train.apply(\n",
    "    lambda row: 1 - calculate_compressed_size(row['text'] + row['prompt_text']) / (calculate_compressed_size(row['text']) + calculate_compressed_size(row['prompt_text'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "if 'semantic_similarity' not in df_train.columns:\n",
    "    df_train['semantic_similarity'] = 0.0\n",
    "\n",
    "if 'semantic_similarity' not in df_test.columns:\n",
    "    df_test['semantic_similarity'] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5623ced",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T14:58:06.913169Z",
     "iopub.status.busy": "2023-08-26T14:58:06.912798Z",
     "iopub.status.idle": "2023-08-26T15:13:32.409892Z",
     "shell.execute_reply": "2023-08-26T15:13:32.408827Z"
    },
    "papermill": {
     "duration": 925.512771,
     "end_time": "2023-08-26T15:13:32.412437",
     "exception": false,
     "start_time": "2023-08-26T14:58:06.899666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def rouge_n(hypothesis, reference, n=1):\n",
    "    hyp_ngrams = set(ngrams(hypothesis.split(), n))\n",
    "    ref_ngrams = set(ngrams(reference.split(), n))\n",
    "    overlap = len(hyp_ngrams.intersection(ref_ngrams))\n",
    "    precision = overlap / len(hyp_ngrams) if hyp_ngrams else 0\n",
    "    recall = overlap / len(ref_ngrams) if ref_ngrams else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "def lcs(X, Y):\n",
    "    m, n = len(X), len(Y)\n",
    "    L = [[0] * (n + 1) for i in range(m + 1)]\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0:\n",
    "                L[i][j] = 0\n",
    "            elif X[i - 1] == Y[j - 1]:\n",
    "                L[i][j] = L[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
    "    return L[m][n]\n",
    "\n",
    "def rouge_l(hypothesis, reference):\n",
    "    lcs_length = lcs(hypothesis.split(), reference.split())\n",
    "    precision = lcs_length / len(hypothesis.split()) if hypothesis else 0\n",
    "    recall = lcs_length / len(reference.split()) if reference else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "def skip_bigrams(tokens):\n",
    "    return [(tokens[i], tokens[j]) for i in range(len(tokens)) for j in range(i+1, len(tokens))]\n",
    "\n",
    "def rouge_s(hypothesis, reference):\n",
    "    hyp_skip_bigrams = set(skip_bigrams(hypothesis.split()))\n",
    "    ref_skip_bigrams = set(skip_bigrams(reference.split()))\n",
    "    overlap = len(hyp_skip_bigrams.intersection(ref_skip_bigrams))\n",
    "    precision = overlap / len(hyp_skip_bigrams) if hyp_skip_bigrams else 0\n",
    "    recall = overlap / len(ref_skip_bigrams) if ref_skip_bigrams else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "def add_scores(row):\n",
    "    hyp = row['text']\n",
    "    ref = row['prompt_text']\n",
    "    scores_1 = rouge_n(hyp, ref, 1)\n",
    "    scores_2 = rouge_n(hyp, ref, 2)\n",
    "    scores_l = rouge_l(hyp, ref)\n",
    "    scores_s = rouge_s(hyp, ref)\n",
    "    return pd.Series({\n",
    "        'rouge_1_f1': scores_1['f1'],\n",
    "        'rouge_2_f1': scores_2['f1'],\n",
    "        'rouge_l_f1': scores_l['f1'],\n",
    "        'rouge_s_f1': scores_s['f1'],\n",
    "        'rouge_1_precision': scores_1['precision'],\n",
    "        'rouge_2_precision': scores_2['precision'],\n",
    "        'rouge_l_precision': scores_l['precision'],\n",
    "        'rouge_s_precision': scores_s['precision'],\n",
    "        'rouge_1_recall': scores_1['recall'],\n",
    "        'rouge_2_recall': scores_2['recall'],\n",
    "        'rouge_l_recall': scores_l['recall'],\n",
    "        'rouge_s_recall': scores_s['recall'],\n",
    "    })\n",
    "\n",
    "\n",
    "ROUGE_COLUMNS = [\n",
    "    'rouge_1_f1', 'rouge_1_precision', 'rouge_1_recall',\n",
    "    'rouge_2_f1', 'rouge_2_precision', 'rouge_2_recall',\n",
    "    'rouge_l_f1', 'rouge_l_precision', 'rouge_l_recall',\n",
    "    'rouge_s_f1', 'rouge_s_precision', 'rouge_s_recall'\n",
    "]\n",
    "\n",
    "df_train = pd.concat([df_train, df_train.apply(add_scores, axis=1)], axis=1)\n",
    "df_test = pd.concat([df_test, df_test.apply(add_scores, axis=1)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d5908e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:13:32.434180Z",
     "iopub.status.busy": "2023-08-26T15:13:32.433880Z",
     "iopub.status.idle": "2023-08-26T15:13:32.455570Z",
     "shell.execute_reply": "2023-08-26T15:13:32.454495Z"
    },
    "id": "50f8787c",
    "outputId": "ca7d8d1a-2468-417e-8799-69397f344c91",
    "papermill": {
     "duration": 0.034837,
     "end_time": "2023-08-26T15:13:32.457642",
     "exception": false,
     "start_time": "2023-08-26T15:13:32.422805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPUs: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    device_ids = list(range(n_gpu))\n",
    "    device = torch.device(f'cuda:{device_ids[0]}')  # Choose the first device\n",
    "    print('There are %d GPU(s) available.' % n_gpu)\n",
    "    print('We will use the GPUs:', device_ids)\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f274b8d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:13:32.479854Z",
     "iopub.status.busy": "2023-08-26T15:13:32.479548Z",
     "iopub.status.idle": "2023-08-26T15:13:32.485096Z",
     "shell.execute_reply": "2023-08-26T15:13:32.484264Z"
    },
    "id": "vQwN7BYiTaBw",
    "papermill": {
     "duration": 0.019014,
     "end_time": "2023-08-26T15:13:32.487173",
     "exception": false,
     "start_time": "2023-08-26T15:13:32.468159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 4\n",
    "DROPOUT = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "MAX_LENGTH = 1024\n",
    "WEIGHT_DECAY = 0.02\n",
    "\n",
    "MODEL_NAME = 'debertav3base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86a4ff22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:13:32.509250Z",
     "iopub.status.busy": "2023-08-26T15:13:32.508506Z",
     "iopub.status.idle": "2023-08-26T15:14:00.191641Z",
     "shell.execute_reply": "2023-08-26T15:14:00.190670Z"
    },
    "id": "acc23ee7",
    "outputId": "684d5b4f-37af-4793-a76e-6e8c2474fe37",
    "papermill": {
     "duration": 27.69695,
     "end_time": "2023-08-26T15:14:00.194033",
     "exception": false,
     "start_time": "2023-08-26T15:13:32.497083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "Some weights of the model checkpoint at /kaggle/input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "def load_pretrained_from_path(model_path, model_name, config_updates):\n",
    "    \"\"\"Load pretrained model and tokenizer from a given path.\"\"\"\n",
    "\n",
    "    full_path = f\"{model_path}/{model_name}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(full_path)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(full_path)\n",
    "    config.update(config_updates)\n",
    "    model = AutoModel.from_pretrained(full_path, config=config)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_pretrained(model_name=MODEL_NAME):\n",
    "\n",
    "    if ENV == \"kaggle\":\n",
    "        MODEL_PATH = '/kaggle/input'\n",
    "    elif ENV == \"colab\":\n",
    "        MODEL_PATH = '/content/drive/MyDrive/kaggle'\n",
    "    else:\n",
    "        raise EnvironmentError(\"Unknown environment.\")\n",
    "\n",
    "    config_updates = {\n",
    "        \"hidden_dropout_prob\": DROPOUT,\n",
    "        \"max_grad_norm\": MAX_GRAD_NORM,\n",
    "        \"weight_decay\": WEIGHT_DECAY\n",
    "    }\n",
    "\n",
    "    return load_pretrained_from_path(MODEL_PATH, model_name, config_updates)\n",
    "\n",
    "model, tokenizer = load_pretrained()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec12e458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:14:00.217359Z",
     "iopub.status.busy": "2023-08-26T15:14:00.217034Z",
     "iopub.status.idle": "2023-08-26T15:33:16.683348Z",
     "shell.execute_reply": "2023-08-26T15:33:16.682263Z"
    },
    "id": "d17e4617",
    "outputId": "f1007a53-edec-40dd-e07d-3538196feab8",
    "papermill": {
     "duration": 1156.480751,
     "end_time": "2023-08-26T15:33:16.685718",
     "exception": false,
     "start_time": "2023-08-26T15:14:00.204967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at /kaggle/input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import hashlib\n",
    "from functools import lru_cache\n",
    "import torch.nn as nn\n",
    "\n",
    "semantic_model, tokenizer = load_pretrained()\n",
    "semantic_model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    semantic_model = nn.DataParallel(semantic_model)\n",
    "\n",
    "maxsize = None if ENV == \"colab\" else 128\n",
    "@lru_cache(maxsize=maxsize)\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, max_length=MAX_LENGTH, truncation=True, padding='max_length', return_tensors='pt')\n",
    "\n",
    "    if device_ids:\n",
    "        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}  # move inputs to GPU\n",
    "    with torch.no_grad():\n",
    "        outputs = semantic_model(**inputs)\n",
    "\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "# Compute embeddings and similarity score\n",
    "def get_similarity(row):\n",
    "    prompt_embedding = get_embedding(row['prompt_text'])\n",
    "    text_embedding = get_embedding(row['text'])\n",
    "    similarity = cosine_similarity(prompt_embedding, text_embedding)\n",
    "    return similarity[0][0]\n",
    "\n",
    "# Add similarity score to DataFrame\n",
    "df_test['semantic_similarity'] = df_test.apply(get_similarity, axis=1)\n",
    "df_train['semantic_similarity'] = df_train.apply(get_similarity, axis=1)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "del semantic_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abadf57",
   "metadata": {
    "id": "9d7d77ff",
    "papermill": {
     "duration": 0.010877,
     "end_time": "2023-08-26T15:33:16.710256",
     "exception": false,
     "start_time": "2023-08-26T15:33:16.699379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e66849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:16.734376Z",
     "iopub.status.busy": "2023-08-26T15:33:16.734064Z",
     "iopub.status.idle": "2023-08-26T15:33:16.789063Z",
     "shell.execute_reply": "2023-08-26T15:33:16.788197Z"
    },
    "id": "96e04d79",
    "papermill": {
     "duration": 0.070169,
     "end_time": "2023-08-26T15:33:16.791222",
     "exception": false,
     "start_time": "2023-08-26T15:33:16.721053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Features you want to scale\n",
    "features_to_scale = [\"word_count\", \"semantic_similarity\", \"lexical_similarity\"]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler only on the training data for the selected features\n",
    "df_train[features_to_scale] = scaler.fit_transform(df_train[features_to_scale])\n",
    "\n",
    "# Transform the test data using the same scaler for the selected features\n",
    "df_test[features_to_scale] = scaler.transform(df_test[features_to_scale])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cefbb9",
   "metadata": {
    "id": "f6ec1996",
    "papermill": {
     "duration": 0.010656,
     "end_time": "2023-08-26T15:33:16.812947",
     "exception": false,
     "start_time": "2023-08-26T15:33:16.802291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98c1fd02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:16.836501Z",
     "iopub.status.busy": "2023-08-26T15:33:16.836215Z",
     "iopub.status.idle": "2023-08-26T15:33:16.844066Z",
     "shell.execute_reply": "2023-08-26T15:33:16.843142Z"
    },
    "id": "3cee70d4",
    "papermill": {
     "duration": 0.022166,
     "end_time": "2023-08-26T15:33:16.846235",
     "exception": false,
     "start_time": "2023-08-26T15:33:16.824069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "def split_dataframe_by_prompt(df, n_splits=4):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    combined_train_dfs = []\n",
    "    combined_val_dfs = []\n",
    "\n",
    "    unique_prompts = df['prompt_id'].unique()\n",
    "\n",
    "    for train_prompt_ids, val_prompt_ids in kf.split(unique_prompts):\n",
    "        train_dataframes = [df[df['prompt_id'] == unique_prompts[id]] for id in train_prompt_ids]\n",
    "        val_dataframes = [df[df['prompt_id'] == unique_prompts[id]] for id in val_prompt_ids]\n",
    "\n",
    "        combined_train_dfs.append(pd.concat(train_dataframes))\n",
    "        combined_val_dfs.append(pd.concat(val_dataframes))\n",
    "\n",
    "    return combined_train_dfs, combined_val_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f436299",
   "metadata": {
    "id": "2b727d53",
    "papermill": {
     "duration": 0.010578,
     "end_time": "2023-08-26T15:33:16.867460",
     "exception": false,
     "start_time": "2023-08-26T15:33:16.856882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b342ec0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:16.890874Z",
     "iopub.status.busy": "2023-08-26T15:33:16.890575Z",
     "iopub.status.idle": "2023-08-26T15:33:16.896501Z",
     "shell.execute_reply": "2023-08-26T15:33:16.895499Z"
    },
    "id": "0017d9e9",
    "papermill": {
     "duration": 0.020284,
     "end_time": "2023-08-26T15:33:16.898770",
     "exception": false,
     "start_time": "2023-08-26T15:33:16.878486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_mcrmse(df):\n",
    "    def rmse(actual, predictions):\n",
    "        return np.sqrt(np.mean((actual - predictions)**2))\n",
    "\n",
    "    rmse_content = rmse(df['content'], df['predicted_content'])\n",
    "    rmse_wording = rmse(df['wording'], df['predicted_wording'])\n",
    "\n",
    "    mcrmse = np.mean([rmse_content, rmse_wording])\n",
    "\n",
    "    return mcrmse, rmse_content, rmse_wording"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9daa97d",
   "metadata": {
    "id": "8917dfd7",
    "papermill": {
     "duration": 0.010747,
     "end_time": "2023-08-26T15:33:16.920188",
     "exception": false,
     "start_time": "2023-08-26T15:33:16.909441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ac59dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:16.943487Z",
     "iopub.status.busy": "2023-08-26T15:33:16.943229Z",
     "iopub.status.idle": "2023-08-26T15:33:16.962949Z",
     "shell.execute_reply": "2023-08-26T15:33:16.962036Z"
    },
    "id": "161a840b",
    "papermill": {
     "duration": 0.033814,
     "end_time": "2023-08-26T15:33:16.964861",
     "exception": false,
     "start_time": "2023-08-26T15:33:16.931047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text = row['full_text']\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs['input_ids'] = inputs['input_ids'].squeeze()\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].squeeze()\n",
    "        return inputs, torch.tensor([row['content'], row['wording']], dtype=torch.float)\n",
    "\n",
    "\n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, dim = 1, p=3, eps=1e-6):\n",
    "        super(GeMText, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n",
    "        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret\n",
    "\n",
    "# https://raphaelb.org/posts/freezing-bert/\n",
    "def partial_freeze(model, frozen_count=4):\n",
    "    for param in model.encoder.layer[:frozen_count].parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.encoder.layer[frozen_count:].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        # Safely retrieve the hidden size\n",
    "        embedding_size = getattr(base_model.config, 'hidden_size', None)\n",
    "        if embedding_size is None:\n",
    "            raise ValueError(f\"'hidden_size' not found in config of model {type(base_model)}\")\n",
    "        self.gem_pooler = GeMText()\n",
    "        self.regressor = nn.Linear(embedding_size, 2)\n",
    "        self._init_weights(self.regressor)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        outputs = self.base_model(**inputs)\n",
    "        sentence_embedding = self.gem_pooler(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "        return self.regressor(sentence_embedding)\n",
    "\n",
    "\n",
    "class MCRMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')  # We will handle the mean computation manually\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        squared_errors = self.mse(pred, target)\n",
    "        rmse_per_column = torch.sqrt(torch.mean(squared_errors, dim=0))\n",
    "        mcrmse = torch.mean(rmse_per_column)\n",
    "        return mcrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a6eb1d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:16.988936Z",
     "iopub.status.busy": "2023-08-26T15:33:16.988185Z",
     "iopub.status.idle": "2023-08-26T15:33:16.996954Z",
     "shell.execute_reply": "2023-08-26T15:33:16.996107Z"
    },
    "id": "a2ee666d",
    "papermill": {
     "duration": 0.022917,
     "end_time": "2023-08-26T15:33:16.998890",
     "exception": false,
     "start_time": "2023-08-26T15:33:16.975973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_with_transformer(model, tokenizer, dataframe, batch_size=4):\n",
    "    \"\"\"Use the transformer model to make predictions in batches.\"\"\"\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    texts = dataframe['full_text'].tolist()\n",
    "    total_samples = len(texts)\n",
    "\n",
    "    predicted_content = []\n",
    "    predicted_wording = []\n",
    "\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        inputs = tokenizer(texts[i:i+batch_size], truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=MAX_LENGTH)\n",
    "\n",
    "        # Send all inputs to device (e.g., GPU)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        predicted_content.extend(outputs[:, 0].cpu().numpy())\n",
    "        predicted_wording.extend(outputs[:, 1].cpu().numpy())\n",
    "\n",
    "    return predicted_content, predicted_wording\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2da8bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:17.024706Z",
     "iopub.status.busy": "2023-08-26T15:33:17.023808Z",
     "iopub.status.idle": "2023-08-26T15:33:17.029424Z",
     "shell.execute_reply": "2023-08-26T15:33:17.028401Z"
    },
    "id": "gUPturB_vZZJ",
    "papermill": {
     "duration": 0.020956,
     "end_time": "2023-08-26T15:33:17.031604",
     "exception": false,
     "start_time": "2023-08-26T15:33:17.010648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_loss(model, tokenizer, dataframe):\n",
    "    dataframe['predicted_content'], dataframe['predicted_wording'] = predict_with_transformer(model, tokenizer, dataframe)\n",
    "    print(compute_mcrmse(dataframe))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdd89e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:17.058474Z",
     "iopub.status.busy": "2023-08-26T15:33:17.058090Z",
     "iopub.status.idle": "2023-08-26T15:33:17.086859Z",
     "shell.execute_reply": "2023-08-26T15:33:17.085777Z"
    },
    "id": "51d3fe8f",
    "papermill": {
     "duration": 0.045711,
     "end_time": "2023-08-26T15:33:17.089456",
     "exception": false,
     "start_time": "2023-08-26T15:33:17.043745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def train_phase(model, dataloader, optimizer, epochs, frozen_count, df_val=None):\n",
    "    partial_freeze(model.base_model, frozen_count=frozen_count)\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    warmup_steps = int(total_steps * 0.1)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    loss_history = []\n",
    "    running_loss = 0.0\n",
    "    loss_func = MCRMSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            current_loss = loss.item()\n",
    "            running_loss += current_loss\n",
    "            loss_history.append(current_loss)\n",
    "            if i % 10 == 9:\n",
    "                avg_loss = running_loss / 10\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "                running_loss = 0.0\n",
    "            scheduler.step()\n",
    "        if df_val is not None:\n",
    "            print_loss(model, tokenizer, df_val)\n",
    "    return loss_history\n",
    "\n",
    "\n",
    "def train_model(df_train, df_val=None):\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    base_model, tokenizer = load_pretrained()\n",
    "\n",
    "    # Set dropout probability\n",
    "    base_model.config.hidden_dropout_prob = DROPOUT\n",
    "    base_model.config.attention_probs_dropout_prob = DROPOUT\n",
    "\n",
    "    # Create the regression model\n",
    "    model = RegressionModel(base_model)\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare the data\n",
    "    dataset_train = TextDataset(df_train, tokenizer, max_length=MAX_LENGTH)\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Set up the optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    loss_history_phase2 = train_phase(model, dataloader_train, optimizer, epochs=3, frozen_count=4, df_val=df_val)\n",
    "\n",
    "    loss_history = loss_history_phase2\n",
    "    return model, loss_history\n",
    "\n",
    "\n",
    "    return model, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b9002de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:17.116811Z",
     "iopub.status.busy": "2023-08-26T15:33:17.115674Z",
     "iopub.status.idle": "2023-08-26T15:33:29.125077Z",
     "shell.execute_reply": "2023-08-26T15:33:29.124017Z"
    },
    "id": "Num2b8Ka-rWw",
    "outputId": "2acfda53-e5fe-4874-d48f-b59b7e3b284a",
    "papermill": {
     "duration": 12.025994,
     "end_time": "2023-08-26T15:33:29.127855",
     "exception": false,
     "start_time": "2023-08-26T15:33:17.101861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at /kaggle/input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if ENV == \"colab\":\n",
    "    model, train_loss = train_model(df_train)\n",
    "    torch.save(model.state_dict(), 'drive/MyDrive/kaggle/colab/tuned_roberta.pth')\n",
    "else:\n",
    "    base_model, _ = load_pretrained()\n",
    "    model = RegressionModel(base_model).to(device)\n",
    "    model.load_state_dict(torch.load('/kaggle/input/tuned-roberta/tuned_roberta.pth', map_location=device), strict=False) # WARNING: strict false may hide bug!\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15751fd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:29.154810Z",
     "iopub.status.busy": "2023-08-26T15:33:29.154434Z",
     "iopub.status.idle": "2023-08-26T15:33:29.166833Z",
     "shell.execute_reply": "2023-08-26T15:33:29.165707Z"
    },
    "id": "_XbprskgRuu5",
    "papermill": {
     "duration": 0.028972,
     "end_time": "2023-08-26T15:33:29.169455",
     "exception": false,
     "start_time": "2023-08-26T15:33:29.140483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "def create_kaggle_dataset(file_path, dataset_name):\n",
    "    if ENV != \"colab\":\n",
    "        print(\"Environment is not Colab. Exiting function.\")\n",
    "        return\n",
    "\n",
    "    YOUR_USERNAME = \"louissanna\"\n",
    "    now = datetime.now()\n",
    "    formatted_date = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "    # Upload kaggle.json if not present\n",
    "    if not os.path.exists('kaggle.json'):\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "\n",
    "    # List of shell commands for initial setup\n",
    "    commands = [\n",
    "        \"pip install kaggle\",\n",
    "        \"mkdir -p ~/.kaggle\",\n",
    "        \"cp kaggle.json ~/.kaggle/\",\n",
    "        \"chmod 600 ~/.kaggle/kaggle.json\",\n",
    "    ]\n",
    "\n",
    "    # Only create 'my_temp_dir' if it doesn't exist\n",
    "    if os.path.exists('my_temp_dir'):\n",
    "        shutil.rmtree('my_temp_dir')\n",
    "    if not os.path.exists('my_temp_dir'):\n",
    "        commands.append(\"mkdir my_temp_dir\")\n",
    "\n",
    "    # Only move the file if it's not already in 'my_temp_dir'\n",
    "    filename = os.path.basename(file_path)\n",
    "    if not os.path.exists(f'my_temp_dir/{filename}'):\n",
    "        commands.append(f\"cp {file_path} my_temp_dir/\")\n",
    "\n",
    "    # Execute each initial setup command\n",
    "    for command in commands:\n",
    "        get_ipython().system(command)\n",
    "\n",
    "    # Check if 'dataset-metadata.json' doesn't exist, then set it\n",
    "    if not os.path.exists(\"my_temp_dir/dataset-metadata.json\"):\n",
    "        metadata = {\n",
    "            \"title\": dataset_name + \"-\" + formatted_date,\n",
    "            \"id\": YOUR_USERNAME + \"/\" + dataset_name + \"-\" + formatted_date,\n",
    "            \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
    "        }\n",
    "\n",
    "        with open(\"my_temp_dir/dataset-metadata.json\", \"w\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "\n",
    "    # Create the dataset on Kaggle\n",
    "    get_ipython().system(\"kaggle datasets create -p my_temp_dir/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c71f777d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:29.196765Z",
     "iopub.status.busy": "2023-08-26T15:33:29.196366Z",
     "iopub.status.idle": "2023-08-26T15:33:29.201818Z",
     "shell.execute_reply": "2023-08-26T15:33:29.200657Z"
    },
    "id": "eUPifBqkeE-1",
    "papermill": {
     "duration": 0.021773,
     "end_time": "2023-08-26T15:33:29.204058",
     "exception": false,
     "start_time": "2023-08-26T15:33:29.182285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ENV == \"colab\":\n",
    "    file_path = 'drive/MyDrive/kaggle/colab/tuned_roberta.pth'\n",
    "    dataset_name = 'tuned-roberta'\n",
    "\n",
    "    create_kaggle_dataset(file_path, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33b00bcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:29.230750Z",
     "iopub.status.busy": "2023-08-26T15:33:29.230385Z",
     "iopub.status.idle": "2023-08-26T15:33:29.236253Z",
     "shell.execute_reply": "2023-08-26T15:33:29.235280Z"
    },
    "id": "998d31a7",
    "outputId": "dae9e48c-d72f-4db3-a335-1c76a831f61d",
    "papermill": {
     "duration": 0.021882,
     "end_time": "2023-08-26T15:33:29.238627",
     "exception": false,
     "start_time": "2023-08-26T15:33:29.216745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if ENV == \"colab\":\n",
    "    # Plotting the loss history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.xlabel('Every Batche')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f81b6",
   "metadata": {
    "id": "2d334c8d",
    "papermill": {
     "duration": 0.012505,
     "end_time": "2023-08-26T15:33:29.263781",
     "exception": false,
     "start_time": "2023-08-26T15:33:29.251276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1ae1ae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:29.290956Z",
     "iopub.status.busy": "2023-08-26T15:33:29.290563Z",
     "iopub.status.idle": "2023-08-26T15:33:36.080732Z",
     "shell.execute_reply": "2023-08-26T15:33:36.079632Z"
    },
    "id": "cd6b9032",
    "outputId": "5a8d1565-bda3-4642-8418-efd123af0ea0",
    "papermill": {
     "duration": 6.806804,
     "end_time": "2023-08-26T15:33:36.083363",
     "exception": false,
     "start_time": "2023-08-26T15:33:29.276559",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at /kaggle/input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_, tokenizer = load_pretrained()\n",
    "\n",
    "if ENV == \"kaggle\":\n",
    "    df_test['transformer_content'], df_test['transformer_wording'] = predict_with_transformer(model, tokenizer, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd6727c",
   "metadata": {
    "id": "5a9e6af5",
    "papermill": {
     "duration": 0.013276,
     "end_time": "2023-08-26T15:33:36.110725",
     "exception": false,
     "start_time": "2023-08-26T15:33:36.097449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4 fold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b281e56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:36.139652Z",
     "iopub.status.busy": "2023-08-26T15:33:36.138489Z",
     "iopub.status.idle": "2023-08-26T15:33:36.149029Z",
     "shell.execute_reply": "2023-08-26T15:33:36.147971Z"
    },
    "id": "0DZfzBkm-Okd",
    "outputId": "d66f7715-998e-414d-bd2c-0f1784b3faa1",
    "papermill": {
     "duration": 0.027478,
     "end_time": "2023-08-26T15:33:36.151134",
     "exception": false,
     "start_time": "2023-08-26T15:33:36.123656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_validation_training(df_train, tokenizer):\n",
    "    train_4_fold_dfs, val_4_fold_dfs = split_dataframe_by_prompt(df_train)\n",
    "    validation_losses = []\n",
    "    oof_predictions = pd.DataFrame() # Initialize an empty DataFrame to store OOF predictions\n",
    "\n",
    "    for fold_number in range(4):\n",
    "        print(f\"Fold {fold_number}\")\n",
    "\n",
    "        df_4_fold_train = train_4_fold_dfs[fold_number]\n",
    "        df_4_fold_val = val_4_fold_dfs[fold_number]\n",
    "\n",
    "        model, _ = train_model(df_4_fold_train, df_val=df_4_fold_val)\n",
    "        df_4_fold_val['predicted_content'], df_4_fold_val['predicted_wording'] = predict_with_transformer(model, tokenizer, df_4_fold_val)\n",
    "\n",
    "        oof_predictions = pd.concat([oof_predictions, df_4_fold_val]) # Add the predictions to OOF DataFrame\n",
    "\n",
    "        import gc\n",
    "        del model ; gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        validation_loss = compute_mcrmse(df_4_fold_val)\n",
    "        validation_losses.append(validation_loss)\n",
    "\n",
    "        print(f\"Validation Loss for Fold {fold_number}: {validation_loss}\")\n",
    "\n",
    "    print(\"Average loss across all folds:\")\n",
    "    print(np.mean(validation_losses))\n",
    "\n",
    "    return oof_predictions, validation_losses\n",
    "\n",
    "if ENV == \"colab\":\n",
    "    oof_predictions, validation_losses = cross_validation_training(df_train, tokenizer)\n",
    "    print(compute_mcrmse(oof_predictions))\n",
    "    oof_predictions[\"transformer_content\"] = oof_predictions[\"predicted_content\"]\n",
    "    oof_predictions[\"transformer_wording\"] = oof_predictions[\"predicted_wording\"]\n",
    "    oof_predictions.to_csv('drive/MyDrive/kaggle/colab/oof_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dec86557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:36.177941Z",
     "iopub.status.busy": "2023-08-26T15:33:36.177599Z",
     "iopub.status.idle": "2023-08-26T15:33:36.182663Z",
     "shell.execute_reply": "2023-08-26T15:33:36.181572Z"
    },
    "id": "4NBlh85v5hNu",
    "outputId": "0d01407f-9d9b-49d1-ffe8-7c0909028852",
    "papermill": {
     "duration": 0.021013,
     "end_time": "2023-08-26T15:33:36.184870",
     "exception": false,
     "start_time": "2023-08-26T15:33:36.163857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'drive/MyDrive/kaggle/colab/oof_predictions.csv'\n",
    "dataset_name = 'summaries-features'\n",
    "\n",
    "if ENV == 'colab':\n",
    "    create_kaggle_dataset(file_path, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c578e3ba",
   "metadata": {
    "id": "HLBDqBxUI784",
    "papermill": {
     "duration": 0.01312,
     "end_time": "2023-08-26T15:33:36.211014",
     "exception": false,
     "start_time": "2023-08-26T15:33:36.197894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a80a25fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:36.238659Z",
     "iopub.status.busy": "2023-08-26T15:33:36.238216Z",
     "iopub.status.idle": "2023-08-26T15:33:39.941437Z",
     "shell.execute_reply": "2023-08-26T15:33:39.939828Z"
    },
    "id": "aF7r2QBkI6XX",
    "outputId": "7688dc33-a598-4a89-8b52-6b2e3a39c33b",
    "papermill": {
     "duration": 3.719957,
     "end_time": "2023-08-26T15:33:39.943729",
     "exception": false,
     "start_time": "2023-08-26T15:33:36.223772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Mean Squared Error for target 'content': 0.17330497596622332\n",
      "Feature lexical_similarity: 5\n",
      "Feature semantic_similarity: 33\n",
      "Feature rouge_1_f1: 0\n",
      "Feature rouge_1_precision: 6\n",
      "Feature rouge_1_recall: 0\n",
      "Feature rouge_2_f1: 1\n",
      "Feature rouge_2_precision: 1\n",
      "Feature rouge_2_recall: 4\n",
      "Feature rouge_l_f1: 5\n",
      "Feature rouge_l_precision: 3\n",
      "Feature rouge_l_recall: 1\n",
      "Feature rouge_s_f1: 4\n",
      "Feature rouge_s_precision: 0\n",
      "Feature rouge_s_recall: 0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Mean Squared Error for target 'content': 0.2741369138845592\n",
      "Feature lexical_similarity: 0\n",
      "Feature semantic_similarity: 2\n",
      "Feature rouge_1_f1: 0\n",
      "Feature rouge_1_precision: 0\n",
      "Feature rouge_1_recall: 0\n",
      "Feature rouge_2_f1: 0\n",
      "Feature rouge_2_precision: 2\n",
      "Feature rouge_2_recall: 0\n",
      "Feature rouge_l_f1: 0\n",
      "Feature rouge_l_precision: 2\n",
      "Feature rouge_l_recall: 0\n",
      "Feature rouge_s_f1: 0\n",
      "Feature rouge_s_precision: 0\n",
      "Feature rouge_s_recall: 0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Mean Squared Error for target 'content': 0.1899810680305828\n",
      "Feature lexical_similarity: 1\n",
      "Feature semantic_similarity: 1\n",
      "Feature rouge_1_f1: 0\n",
      "Feature rouge_1_precision: 0\n",
      "Feature rouge_1_recall: 0\n",
      "Feature rouge_2_f1: 0\n",
      "Feature rouge_2_precision: 3\n",
      "Feature rouge_2_recall: 0\n",
      "Feature rouge_l_f1: 0\n",
      "Feature rouge_l_precision: 0\n",
      "Feature rouge_l_recall: 0\n",
      "Feature rouge_s_f1: 0\n",
      "Feature rouge_s_precision: 0\n",
      "Feature rouge_s_recall: 1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Mean Squared Error for target 'content': 0.3151697606627338\n",
      "Feature lexical_similarity: 0\n",
      "Feature semantic_similarity: 4\n",
      "Feature rouge_1_f1: 0\n",
      "Feature rouge_1_precision: 0\n",
      "Feature rouge_1_recall: 0\n",
      "Feature rouge_2_f1: 0\n",
      "Feature rouge_2_precision: 2\n",
      "Feature rouge_2_recall: 0\n",
      "Feature rouge_l_f1: 0\n",
      "Feature rouge_l_precision: 1\n",
      "Feature rouge_l_recall: 0\n",
      "Feature rouge_s_f1: 0\n",
      "Feature rouge_s_precision: 0\n",
      "Feature rouge_s_recall: 0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Mean Squared Error for target 'wording': 0.3850383241607603\n",
      "Feature lexical_similarity: 37\n",
      "Feature semantic_similarity: 65\n",
      "Feature rouge_1_f1: 0\n",
      "Feature rouge_1_precision: 38\n",
      "Feature rouge_1_recall: 5\n",
      "Feature rouge_2_f1: 6\n",
      "Feature rouge_2_precision: 3\n",
      "Feature rouge_2_recall: 4\n",
      "Feature rouge_l_f1: 0\n",
      "Feature rouge_l_precision: 24\n",
      "Feature rouge_l_recall: 0\n",
      "Feature rouge_s_f1: 0\n",
      "Feature rouge_s_precision: 8\n",
      "Feature rouge_s_recall: 6\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Mean Squared Error for target 'wording': 0.5337638419431079\n",
      "Feature lexical_similarity: 6\n",
      "Feature semantic_similarity: 80\n",
      "Feature rouge_1_f1: 5\n",
      "Feature rouge_1_precision: 30\n",
      "Feature rouge_1_recall: 0\n",
      "Feature rouge_2_f1: 10\n",
      "Feature rouge_2_precision: 15\n",
      "Feature rouge_2_recall: 1\n",
      "Feature rouge_l_f1: 5\n",
      "Feature rouge_l_precision: 26\n",
      "Feature rouge_l_recall: 2\n",
      "Feature rouge_s_f1: 5\n",
      "Feature rouge_s_precision: 7\n",
      "Feature rouge_s_recall: 4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Mean Squared Error for target 'wording': 0.2557434394824051\n",
      "Feature lexical_similarity: 11\n",
      "Feature semantic_similarity: 24\n",
      "Feature rouge_1_f1: 0\n",
      "Feature rouge_1_precision: 16\n",
      "Feature rouge_1_recall: 0\n",
      "Feature rouge_2_f1: 3\n",
      "Feature rouge_2_precision: 7\n",
      "Feature rouge_2_recall: 0\n",
      "Feature rouge_l_f1: 1\n",
      "Feature rouge_l_precision: 3\n",
      "Feature rouge_l_recall: 0\n",
      "Feature rouge_s_f1: 0\n",
      "Feature rouge_s_precision: 0\n",
      "Feature rouge_s_recall: 5\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Mean Squared Error for target 'wording': 0.44895242609744235\n",
      "Feature lexical_similarity: 25\n",
      "Feature semantic_similarity: 88\n",
      "Feature rouge_1_f1: 0\n",
      "Feature rouge_1_precision: 33\n",
      "Feature rouge_1_recall: 2\n",
      "Feature rouge_2_f1: 4\n",
      "Feature rouge_2_precision: 1\n",
      "Feature rouge_2_recall: 0\n",
      "Feature rouge_l_f1: 1\n",
      "Feature rouge_l_precision: 5\n",
      "Feature rouge_l_recall: 0\n",
      "Feature rouge_s_f1: 2\n",
      "Feature rouge_s_precision: 18\n",
      "Feature rouge_s_recall: 2\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def train_models_with_folds(train_folds, val_folds, feature_columns, target_column, init_column):\n",
    "    models = []\n",
    "    for train_df, val_df in zip(train_folds, val_folds):\n",
    "        model = lgb.LGBMRegressor(learning_rate=0.05, max_depth=3, lambda_l2=0.01, n_estimators=10000)\n",
    "\n",
    "        early_stopping_callback = lgb.early_stopping(stopping_rounds=30, verbose=False)\n",
    "        \n",
    "        # Extract the feature values for the initial prediction\n",
    "        init_score_train = train_df[init_column].values\n",
    "        init_score_val = val_df[init_column].values\n",
    "\n",
    "        model.fit(train_df[feature_columns], train_df[target_column],\n",
    "                  eval_set=[(val_df[feature_columns], val_df[target_column])],\n",
    "                  callbacks=[early_stopping_callback],\n",
    "                  init_score=init_score_train,\n",
    "                  eval_init_score=[init_score_val]\n",
    "                  )\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "\n",
    "        # Predict on validation set and calculate error\n",
    "        val_predictions = model.predict(val_df[feature_columns])\n",
    "        mse = mean_squared_error(val_df[target_column], val_predictions + val_df[init_column])\n",
    "        print(f\"Mean Squared Error for target '{target_column}': {mse}\")\n",
    "        feature_importance = model.feature_importances_\n",
    "        # Print feature importance\n",
    "        for name, importance in zip(feature_columns, feature_importance):\n",
    "            print(f\"Feature {name}: {importance}\")\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def predict_with_models(models, df, feature_columns, init_column):\n",
    "    init_score_test = df[init_column].values\n",
    "    predictions = [model.predict(df[feature_columns]) + init_score_test for model in models]\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "\n",
    "train_4_fold_dfs, val_4_fold_dfs = split_dataframe_by_prompt(df_train)\n",
    "\n",
    "# RMK: transformer_content col will be exploited by the model as initial pred\n",
    "content_feature_columns = ['lexical_similarity', 'semantic_similarity'] + ROUGE_COLUMNS\n",
    "content_models = train_models_with_folds(train_4_fold_dfs, val_4_fold_dfs, content_feature_columns, \"content\", \"transformer_content\")\n",
    "df_test[\"content\"] = predict_with_models(content_models, df_test, content_feature_columns, \"transformer_content\")\n",
    "\n",
    "# RMK: transformer_wording col will be exploited by the model as initial pred\n",
    "wording_feature_columns = ['lexical_similarity', 'semantic_similarity'] + ROUGE_COLUMNS\n",
    "content_models = train_models_with_folds(train_4_fold_dfs, val_4_fold_dfs, wording_feature_columns, \"wording\", \"transformer_wording\")\n",
    "df_test[\"wording\"] = predict_with_models(content_models, df_test, wording_feature_columns, \"transformer_wording\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba397b",
   "metadata": {
    "id": "1b8d3cad",
    "papermill": {
     "duration": 0.013066,
     "end_time": "2023-08-26T15:33:39.971273",
     "exception": false,
     "start_time": "2023-08-26T15:33:39.958207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8fd5bb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T15:33:39.998798Z",
     "iopub.status.busy": "2023-08-26T15:33:39.997923Z",
     "iopub.status.idle": "2023-08-26T15:33:40.025647Z",
     "shell.execute_reply": "2023-08-26T15:33:40.024415Z"
    },
    "id": "8f2148c9",
    "outputId": "4f748413-6c5a-4eb0-f9cd-555e60bc6740",
    "papermill": {
     "duration": 0.044058,
     "end_time": "2023-08-26T15:33:40.027973",
     "exception": false,
     "start_time": "2023-08-26T15:33:39.983915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test.shape: (4, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>-1.182625</td>\n",
       "      <td>-0.988491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>-1.174274</td>\n",
       "      <td>-0.978993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>-1.170675</td>\n",
       "      <td>-0.989027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>-1.163676</td>\n",
       "      <td>-0.964203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id   content   wording\n",
       "0  000000ffffff -1.182625 -0.988491\n",
       "1  222222cccccc -1.174274 -0.978993\n",
       "2  111111eeeeee -1.170675 -0.989027\n",
       "3  333333dddddd -1.163676 -0.964203"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"df_test.shape: {df_test.shape}\")\n",
    "\n",
    "def is_valid_float(x):\n",
    "    return isinstance(x, float) and x == x  # This checks that x is not NaN since NaN != NaN in Python.\n",
    "\n",
    "# Replace non-float values in 'wording' and 'content' with zero\n",
    "cols_to_check = ['wording', 'content']\n",
    "df_test[cols_to_check] = df_test[cols_to_check].applymap(lambda x: x if is_valid_float(x) else 0.0)\n",
    "\n",
    "df_test[['student_id', 'content', 'wording']].to_csv('submission.csv',index=False)\n",
    "display(pd.read_csv('submission.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2163.815575,
   "end_time": "2023-08-26T15:33:43.100905",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-26T14:57:39.285330",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
